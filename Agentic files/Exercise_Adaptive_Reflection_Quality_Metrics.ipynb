{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08f3163c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All imports successful\n",
      " LLM initialized: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "from typing import Annotated, TypedDict, List\n",
    "import os\n",
    "import json\n",
    "\n",
    "print(\" All imports successful\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found!\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7, api_key=openai_api_key)\n",
    "print(f\" LLM initialized: {llm.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6511538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pydantic models created\n",
      "   - QualityScore: Structured scoring model\n",
      "   - ReflectionState: Graph state management\n"
     ]
    }
   ],
   "source": [
    "# Pydantic models for structured quality scoring\n",
    "class QualityScore(BaseModel):\n",
    "    \"\"\"Quality assessment for a draft with three criteria.\"\"\"\n",
    "    clarity: int = Field(\n",
    "        ge=1, le=5,\n",
    "        description=\"How clear and easy to understand is the response? 1=confusing, 5=crystal clear\"\n",
    "    )\n",
    "    completeness: int = Field(\n",
    "        ge=1, le=5,\n",
    "        description=\"Does it cover all aspects of the request? 1=incomplete, 5=comprehensive\"\n",
    "    )\n",
    "    accuracy: int = Field(\n",
    "        ge=1, le=5,\n",
    "        description=\"Is the content factually correct? 1=incorrect, 5=highly accurate\"\n",
    "    )\n",
    "    feedback: str = Field(\n",
    "        description=\"Specific feedback on what needs improvement\"\n",
    "    )\n",
    "    \n",
    "    @property\n",
    "    def average_score(self) -> float:\n",
    "        \"\"\"Calculate average score across all criteria.\"\"\"\n",
    "        return (self.clarity + self.completeness + self.accuracy) / 3\n",
    "    \n",
    "    @property\n",
    "    def all_above_threshold(self) -> bool:\n",
    "        \"\"\"Check if all scores are >= 4.\"\"\"\n",
    "        return all(score >= 4 for score in [self.clarity, self.completeness, self.accuracy])\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Pretty print the scores.\"\"\"\n",
    "        avg = self.average_score\n",
    "        threshold_status = \" PASS\" if self.all_above_threshold else \" NEEDS REFINEMENT\"\n",
    "        return f\"\"\"Quality Scores: {threshold_status}\n",
    "  Clarity:      {self.clarity}/5\n",
    "  Completeness: {self.completeness}/5\n",
    "  Accuracy:     {self.accuracy}/5\n",
    "  Average:      {avg:.2f}/5\n",
    "  Feedback:     {self.feedback}\"\"\"\n",
    "\n",
    "# Define state for the graph\n",
    "class ReflectionState(TypedDict):\n",
    "    \"\"\"State for the adaptive reflection process.\"\"\"\n",
    "    topic: str\n",
    "    draft: str\n",
    "    iteration: int\n",
    "    scores_history: List[QualityScore]\n",
    "    refinement_needed: bool\n",
    "\n",
    "print(\" Pydantic models created\")\n",
    "print(f\"   - QualityScore: Structured scoring model\")\n",
    "print(f\"   - ReflectionState: Graph state management\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9ea54f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Graph nodes defined\n",
      "   - generate_draft: Creates or refines content\n",
      "   - score_draft: Critic evaluation with scoring\n",
      "   - should_refine: Decision logic\n"
     ]
    }
   ],
   "source": [
    "# Node 1: Initial Draft Generation\n",
    "def generate_draft(state: ReflectionState) -> ReflectionState:\n",
    "    \"\"\"Generate initial draft from topic.\"\"\"\n",
    "    if state[\"iteration\"] == 1:\n",
    "        prompt = f\"\"\"You are a professional writer. Write a high-quality response about the following topic:\n",
    "\n",
    "Topic: {state['topic']}\n",
    "\n",
    "Write a clear, comprehensive response (150-250 words) that thoroughly addresses the topic.\"\"\"\n",
    "    else:\n",
    "        # On iterations > 1, refine based on feedback\n",
    "        feedback = state[\"scores_history\"][-1].feedback\n",
    "        prompt = f\"\"\"Improve the following draft based on this feedback:\n",
    "\n",
    "Feedback: {feedback}\n",
    "\n",
    "Original draft:\n",
    "{state['draft']}\n",
    "\n",
    "Write an improved version that addresses the feedback while maintaining the original structure. Keep it 150-250 words.\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    state[\"draft\"] = response.content\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Node 2: Quality Scoring (Critic)\n",
    "def score_draft(state: ReflectionState) -> ReflectionState:\n",
    "    \"\"\"Critic evaluates draft and provides structured scores.\"\"\"\n",
    "    prompt = f\"\"\"You are an expert quality critic. Evaluate this draft on three criteria:\n",
    "1. Clarity (1-5): How clear and well-written is it?\n",
    "2. Completeness (1-5): Does it comprehensively cover the topic?\n",
    "3. Accuracy (1-5): Is the information factually correct?\n",
    "\n",
    "Draft to evaluate:\n",
    "{state['draft']}\n",
    "\n",
    "Respond with JSON containing only these fields:\n",
    "{{\n",
    "  \"clarity\": <int 1-5>,\n",
    "  \"completeness\": <int 1-5>,\n",
    "  \"accuracy\": <int 1-5>,\n",
    "  \"feedback\": \"<string with specific improvement suggestions>\"\n",
    "}}\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    # Parse JSON response\n",
    "    try:\n",
    "        score_dict = json.loads(response.content)\n",
    "        score = QualityScore(**score_dict)\n",
    "    except:\n",
    "        # Fallback if parsing fails\n",
    "        score = QualityScore(\n",
    "            clarity=3,\n",
    "            completeness=3,\n",
    "            accuracy=3,\n",
    "            feedback=\"Unable to parse response\"\n",
    "        )\n",
    "    \n",
    "    state[\"scores_history\"].append(score)\n",
    "    state[\"refinement_needed\"] = not score.all_above_threshold\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Node 3: Refinement Decision\n",
    "def should_refine(state: ReflectionState) -> str:\n",
    "    \"\"\"Decide whether to refine or end.\"\"\"\n",
    "    max_iterations = 3\n",
    "    \n",
    "    if state[\"refinement_needed\"] and state[\"iteration\"] < max_iterations:\n",
    "        return \"refine\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "\n",
    "print(\" Graph nodes defined\")\n",
    "print(\"   - generate_draft: Creates or refines content\")\n",
    "print(\"   - score_draft: Critic evaluation with scoring\")\n",
    "print(\"   - should_refine: Decision logic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0213208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Graph built with conditional routing\n",
      "   Flow: generate ‚Üí score ‚Üí [conditional] ‚Üí update ‚Üí generate (loop) or END\n"
     ]
    }
   ],
   "source": [
    "def update_iteration(state: ReflectionState) -> ReflectionState:\n",
    "    \"\"\"Increment iteration counter.\"\"\"\n",
    "    state[\"iteration\"] += 1\n",
    "    return state\n",
    "\n",
    "# Build the graph\n",
    "builder = StateGraph(ReflectionState)\n",
    "\n",
    "# Add nodes\n",
    "builder.add_node(\"generate\", generate_draft)\n",
    "builder.add_node(\"score\", score_draft)\n",
    "builder.add_node(\"update\", update_iteration)\n",
    "\n",
    "# Define edges\n",
    "builder.add_edge(START, \"generate\")  # Start with draft generation\n",
    "builder.add_edge(\"generate\", \"score\")  # Always score after generating\n",
    "\n",
    "# Conditional edge: refine or end\n",
    "builder.add_conditional_edges(\n",
    "    \"score\",\n",
    "    should_refine,\n",
    "    {\n",
    "        \"refine\": \"update\",      # If refinement needed, increment and loop\n",
    "        \"end\": END               # Otherwise, done\n",
    "    }\n",
    ")\n",
    "\n",
    "# Loop back to generate if refining\n",
    "builder.add_edge(\"update\", \"generate\")\n",
    "\n",
    "# Compile the graph\n",
    "reflection_agent = builder.compile()\n",
    "\n",
    "print(\" Graph built with conditional routing\")\n",
    "print(\"   Flow: generate ‚Üí score ‚Üí [conditional] ‚Üí update ‚Üí generate (loop) or END\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caeecdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEST #1: WHAT IS PYTHON\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      " FINAL RESULT (Iteration 1)\n",
      "================================================================================\n",
      "\n",
      " Final Draft:\n",
      "--------------------------------------------------------------------------------\n",
      "Python is a high-level, interpreted programming language known for its simplicity and readability, making it an excellent choice for both beginners and experienced developers. Created by Guido van Rossum and released in 1991, Python emphasizes code clarity and syntax that allows programmers to express concepts in fewer lines of code compared to languages like C++ or Java.\n",
      "\n",
      "One of Python's key features is its versatility; it supports multiple programming paradigms, including procedural, object-oriented, and functional programming. This flexibility enables developers to tackle a wide range of applications, from web development and data analysis to artificial intelligence and scientific computing. Python's extensive standard library and a rich ecosystem of third-party packages, such as NumPy for numerical computations and Django for web development, broaden its applicability even further.\n",
      "\n",
      "Moreover, Python has a strong community that contributes to its continuous improvement and provides support through forums and documentation. This collaborative environment fosters innovation and keeps the language relevant in an ever-evolving tech landscape.\n",
      "\n",
      "In summary, Python is a powerful and user-friendly language that combines simplicity with versatility, making it a popular choice for developers across various fields. Whether you are building a small script or a complex application, Python provides the tools and frameworks to bring your ideas to life.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " SCORE PROGRESSION (1 iterations):\n",
      "\n",
      "Iteration 1:\n",
      "Quality Scores:  PASS\n",
      "  Clarity:      5/5\n",
      "  Completeness: 4/5\n",
      "  Accuracy:     5/5\n",
      "  Average:      4.67/5\n",
      "  Feedback:     The draft is well-written and clear, making it easy to understand. However, it could benefit from a brief mention of the drawbacks or limitations of Python to provide a more balanced view. Additionally, including more specific examples of applications or use cases could enhance the completeness of the content.\n",
      "\n",
      "================================================================================\n",
      " SUMMARY:\n",
      "  Initial average score: 4.67/5.0\n",
      "  Final average score:   4.67/5.0\n",
      "  Improvement: +0.00 points\n",
      "  Total iterations: 1\n",
      "  Status:  PASSED\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Test session ended\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_count = 0\n",
    "while True:\n",
    "    topic = input(\"üë§ Enter a topic (or 'exit'): \").strip()\n",
    "    \n",
    "    if topic.lower() == \"exit\":\n",
    "        print(\"\\n Test session ended\")\n",
    "        break\n",
    "    \n",
    "    if not topic:\n",
    "        print(\" Please enter a topic\\n\")\n",
    "        continue\n",
    "    \n",
    "    test_count += 1\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TEST #{test_count}: {topic.upper()}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Initialize state\n",
    "    initial_state: ReflectionState = {\n",
    "        \"topic\": topic,\n",
    "        \"draft\": \"\",\n",
    "        \"iteration\": 1,\n",
    "        \"scores_history\": [],\n",
    "        \"refinement_needed\": True\n",
    "    }\n",
    "    \n",
    "    # Run the reflection loop\n",
    "    result = reflection_agent.invoke(initial_state)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\" FINAL RESULT (Iteration {result['iteration']})\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(\" Final Draft:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(result[\"draft\"])\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Display all iterations' scores\n",
    "    print(f\"\\n SCORE PROGRESSION ({len(result['scores_history'])} iterations):\\n\")\n",
    "    \n",
    "    for i, score in enumerate(result[\"scores_history\"], 1):\n",
    "        print(f\"Iteration {i}:\")\n",
    "        print(score)\n",
    "        print()\n",
    "    \n",
    "    # Summary statistics\n",
    "    if result[\"scores_history\"]:\n",
    "        final_score = result[\"scores_history\"][-1]\n",
    "        avg_scores = [\n",
    "            result[\"scores_history\"][0].average_score,\n",
    "            result[\"scores_history\"][-1].average_score\n",
    "        ]\n",
    "        improvement = avg_scores[-1] - avg_scores[0]\n",
    "        \n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\" SUMMARY:\")\n",
    "        print(f\"  Initial average score: {avg_scores[0]:.2f}/5.0\")\n",
    "        print(f\"  Final average score:   {avg_scores[-1]:.2f}/5.0\")\n",
    "        print(f\"  Improvement: {improvement:+.2f} points\")\n",
    "        print(f\"  Total iterations: {len(result['scores_history'])}\")\n",
    "        print(f\"  Status: {' PASSED' if final_score.all_above_threshold else '‚ùå NEEDS MORE WORK'}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
