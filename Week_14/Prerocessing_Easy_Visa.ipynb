{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe78584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import json\n",
    "import joblib\n",
    "# Preprocessing libraries\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Statistical libraries\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore, skew\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "669526eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "url =(\"https://raw.githubusercontent.com/ek-chris/Practice_datasets/refs/heads/main/EasyVisa%20(1).csv\")\n",
    "Easy_Visa = pd.read_csv(url)\n",
    "df_processed = Easy_Visa.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fda3708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocessing(df_processe, target_col='case_status'):\n",
    "  \n",
    "    \n",
    "    print(\"DATA QUALITY ANALYSIS REPORT \")\n",
    "    print(f\"Dataset shape: {df_processe.shape}\")\n",
    "    \n",
    "    #  Missing Values\n",
    "    print(\"\\n. Missing Values:\")\n",
    "    missing_values = df_processe.isnull().sum()\n",
    "    if missing_values.sum() > 0:\n",
    "        print(missing_values[missing_values > 0])\n",
    "    else:\n",
    "        print(\" No missing values found.\")\n",
    "    \n",
    "    #  Duplicate Rows\n",
    "    print(\"\\n. Duplicate Rows:\")\n",
    "    duplicates = df_processe.duplicated().sum()\n",
    "    print(f\"Number of duplicate rows: {duplicates}\")\n",
    "    if duplicates > 0:\n",
    "        print(f\"Percentage of duplicates: {(duplicates/len(df_processe))*100:.2f}%\")\n",
    "    else:\n",
    "        print(\" No duplicate rows found.\")\n",
    "    \n",
    "    #  Skewness for Numerical Variables\n",
    "    print(\"\\n. Skewness Analysis (for numerical features):\")\n",
    "    num_cols = df_processe.select_dtypes(include=['int64', 'float64']).columns\n",
    "    if len(num_cols) > 0:\n",
    "        for col in num_cols:\n",
    "            sk = skew(df_processe[col].dropna())\n",
    "            label = \"right-skewed\" if sk > 0.5 else \"approximately normal\"\n",
    "            print(f\"{col}: skewness = {sk:.3f} ({label})\")\n",
    "    else:\n",
    "        print(\"No numerical columns found.\")\n",
    "    \n",
    "    #  Correlation Among Numerical Features\n",
    "    print(\"\\n. Correlation Analysis:\")\n",
    "    if len(num_cols) > 1:\n",
    "        corr_matrix = df_processe[num_cols].corr(numeric_only=True)\n",
    "        print(corr_matrix)\n",
    "        \n",
    "        # Optional: Correlation with target column if numeric\n",
    "        if target_col in corr_matrix.columns:\n",
    "            correlations = corr_matrix[target_col].sort_values(key=abs, ascending=False)\n",
    "            print(f\"\\nTop correlated features with '{target_col}':\")\n",
    "            print(correlations[1:6])  # exclude target itself\n",
    "    else:\n",
    "        print(\"Not enough numerical columns for correlation analysis.\")\n",
    "    \n",
    "    print(\"\\n Data quality analysis completed successfully.\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1349f0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA QUALITY ANALYSIS REPORT \n",
      "Dataset shape: (25480, 12)\n",
      "\n",
      ". Missing Values:\n",
      " No missing values found.\n",
      "\n",
      ". Duplicate Rows:\n",
      "Number of duplicate rows: 0\n",
      " No duplicate rows found.\n",
      "\n",
      ". Skewness Analysis (for numerical features):\n",
      "no_of_employees: skewness = 12.265 (right-skewed)\n",
      "yr_of_estab: skewness = -2.037 (approximately normal)\n",
      "prevailing_wage: skewness = 0.756 (right-skewed)\n",
      "\n",
      ". Correlation Analysis:\n",
      "                 no_of_employees  yr_of_estab  prevailing_wage\n",
      "no_of_employees         1.000000    -0.017770        -0.009523\n",
      "yr_of_estab            -0.017770     1.000000         0.012342\n",
      "prevailing_wage        -0.009523     0.012342         1.000000\n",
      "\n",
      " Data quality analysis completed successfully.\n"
     ]
    }
   ],
   "source": [
    "df_processed = preprocessing(Easy_Visa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e09dba8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical features:\n",
      " ['no_of_employees', 'yr_of_estab', 'prevailing_wage']\n"
     ]
    }
   ],
   "source": [
    "numerical_features = Easy_Visa.select_dtypes(include = ['int', 'float']).columns.tolist()\n",
    "print(f\"numerical features:\\n {numerical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2ad59c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical features:\n",
      " ['case_id', 'continent', 'education_of_employee', 'has_job_experience', 'requires_job_training', 'region_of_employment', 'unit_of_wage', 'full_time_position', 'case_status']\n"
     ]
    }
   ],
   "source": [
    "categorical_features = Easy_Visa.select_dtypes(include = ['object']). columns.tolist()\n",
    "print(f\"categorical features:\\n {categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c51ca6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_employee_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the employee dataset by encoding categorical variables.\n",
    "    - Binary categorical columns: Label encoded using custom mappings\n",
    "    - Multi-category columns: One-hot encoded\n",
    "    \"\"\"\n",
    "    # Label Encoding (binary columns)\n",
    "    label_map = {\n",
    "        'has_job_experience': {'Y': 1, 'N': 0},\n",
    "        'requires_job_training': {'Y': 1, 'N': 0},\n",
    "        'full_time_position': {'Y': 1, 'N': 0},\n",
    "        'case_status': {'Certified': 1, 'Denied': 0}\n",
    "    }\n",
    "    for col, mapping in label_map.items():\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].map(mapping)\n",
    "    # One-hot Encoding (multi-category columns)\n",
    "    onehot_cols = [\n",
    "        'continent',\n",
    "        'education_of_employee',\n",
    "        'region_of_employment',\n",
    "        'unit_of_wage'\n",
    "    ]\n",
    "    df = pd.get_dummies(df, columns=onehot_cols, drop_first=False, dtype=int)\n",
    "    print(\"\\nPreprocessing complete.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5ea0920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing complete.\n",
      "  case_id  has_job_experience  requires_job_training  no_of_employees  \\\n",
      "0  EZYV01                   0                      0            14513   \n",
      "1  EZYV02                   1                      0             2412   \n",
      "2  EZYV03                   0                      1            44444   \n",
      "3  EZYV04                   0                      0               98   \n",
      "4  EZYV05                   1                      0             1082   \n",
      "\n",
      "   yr_of_estab  prevailing_wage  full_time_position  case_status  \\\n",
      "0         2007         592.2029                   1            0   \n",
      "1         2002       83425.6500                   1            1   \n",
      "2         2008      122996.8600                   1            0   \n",
      "3         1897       83434.0300                   1            0   \n",
      "4         2005      149907.3900                   1            1   \n",
      "\n",
      "   continent_Africa  continent_Asia  ...  education_of_employee_Master's  \\\n",
      "0                 0               1  ...                               0   \n",
      "1                 0               1  ...                               1   \n",
      "2                 0               1  ...                               0   \n",
      "3                 0               1  ...                               0   \n",
      "4                 1               0  ...                               1   \n",
      "\n",
      "   region_of_employment_Island  region_of_employment_Midwest  \\\n",
      "0                            0                             0   \n",
      "1                            0                             0   \n",
      "2                            0                             0   \n",
      "3                            0                             0   \n",
      "4                            0                             0   \n",
      "\n",
      "   region_of_employment_Northeast  region_of_employment_South  \\\n",
      "0                               0                           0   \n",
      "1                               1                           0   \n",
      "2                               0                           0   \n",
      "3                               0                           0   \n",
      "4                               0                           1   \n",
      "\n",
      "   region_of_employment_West  unit_of_wage_Hour  unit_of_wage_Month  \\\n",
      "0                          1                  1                   0   \n",
      "1                          0                  0                   0   \n",
      "2                          1                  0                   0   \n",
      "3                          1                  0                   0   \n",
      "4                          0                  0                   0   \n",
      "\n",
      "   unit_of_wage_Week  unit_of_wage_Year  \n",
      "0                  0                  0  \n",
      "1                  0                  1  \n",
      "2                  0                  1  \n",
      "3                  0                  1  \n",
      "4                  0                  1  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "df_processed = preprocess_employee_data(Easy_Visa)\n",
    "print(df_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f177c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def apply_log_transformation(df, skewed_vars):\n",
    "    \"\"\"\n",
    "    Applies log or log1p transformation to reduce skewness in given numerical columns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The DataFrame containing the features.\n",
    "    skewed_vars : list\n",
    "        List of column names to apply log transformation on.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df : pd.DataFrame\n",
    "        Updated DataFrame with new log-transformed columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== LOG TRANSFORMATION REPORT ===\")\n",
    "    \n",
    "    for var in skewed_vars:\n",
    "        if var in df.columns:\n",
    "            min_val = df[var].min()\n",
    "            \n",
    "            # Handle zero or negative values safely\n",
    "            if min_val <= 0:\n",
    "                df[f'{var}_log'] = np.log1p(df[var])\n",
    "                print(f\" {var}: Applied log1p transformation (min={min_val:.3f})\")\n",
    "            else:\n",
    "                df[f'{var}_log'] = np.log(df[var])\n",
    "                print(f\" {var}: Applied natural log transformation\")\n",
    "            \n",
    "            # Compare skewness before and after\n",
    "            original_skew = skew(df[var].dropna())\n",
    "            transformed_skew = skew(df[f'{var}_log'].dropna())\n",
    "            print(f\"   Original skewness: {original_skew:.3f} â†’ Transformed skewness: {transformed_skew:.3f}\\n\")\n",
    "        else:\n",
    "            print(f\" Column '{var}' not found in DataFrame.\\n\")\n",
    "    \n",
    "    print(f\"Final dataset shape: {df.shape}\")\n",
    "    print(\"New log-transformed columns:\", [col for col in df.columns if '_log' in col])\n",
    "    print(\"=== Transformation Complete ===\\n\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7f1eac",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "apply_log_transformation() missing 1 required positional argument: 'skewed_vars'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_processed =  \u001b[43mapply_log_transformation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_processed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: apply_log_transformation() missing 1 required positional argument: 'skewed_vars'"
     ]
    }
   ],
   "source": [
    "df_processed =  apply_log_transformation(df_processed, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04623d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTLIER TREATMENT (IQR-CAPPING METHOD)\n",
      "EDA recommended IQR-capping for extreme acidity/sulphates to preserve data points\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEDA recommended IQR-capping for extreme acidity/sulphates to preserve data points\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Define numerical columns (excluding target)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m numerical_cols = \u001b[43mdf_processed\u001b[49m.select_dtypes(include=[np.number]).columns.tolist()\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mCase_Status\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m numerical_cols:\n\u001b[32m      8\u001b[39m     numerical_cols.remove(\u001b[33m'\u001b[39m\u001b[33mCase_Status\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df_processed' is not defined"
     ]
    }
   ],
   "source": [
    "# Outlier treatment based on EDA recommendations\n",
    "print(\"OUTLIER TREATMENT (IQR-CAPPING METHOD)\")\n",
    "print(\"EDA recommended IQR-capping for extreme acidity/sulphates to preserve data points\")\n",
    "\n",
    "# Define numerical columns (excluding target)\n",
    "numerical_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'Case_Status' in numerical_cols:\n",
    "    numerical_cols.remove('Case_Status')\n",
    "\n",
    "    print(f\"Treating outliers in {len(numerical_cols)} numerical features\")\n",
    "\n",
    "# Apply IQR-capping method\n",
    "outliers_capped = 0\n",
    "for col in numerical_cols:\n",
    "    Q1 = df_processed[col].quantile(0.25)\n",
    "    Q3 = df_processed[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Count outliers before capping\n",
    "    outliers_before = ((df_processed[col] < lower_bound) | (df_processed[col] > upper_bound)).sum()\n",
    "    \n",
    "    if outliers_before > 0:\n",
    "        # Cap outliers\n",
    "        df_processed[col] = np.where(df_processed[col] < lower_bound, lower_bound, df_processed[col])\n",
    "        df_processed[col] = np.where(df_processed[col] > upper_bound, upper_bound, df_processed[col])\n",
    "        outliers_capped += outliers_before\n",
    "        print(f\" {col}: Capped {outliers_before} outliers\")\n",
    "\n",
    "print(f\"\\nTotal outliers capped: {outliers_capped}\")\n",
    "print(f\"Dataset shape after outlier treatment: {df_processed.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
