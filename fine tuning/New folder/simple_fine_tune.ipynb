{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7262c851",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:❌ Dataset not found: C:\\Users\\ncc333\\Desktop\\My_Task\\fine tuning\\New folder\\fine_tune_dataset.jsonl\n",
      "ERROR:__main__:Please update DATASET_PATH in the script!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SIMPLE FINE-TUNING SCRIPT - All-in-One\n",
    "Just update the paths below and run!\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "# CONFIGURATION - EDIT THESE VALUES\n",
    "\n",
    "# Your dataset path (update this!)\n",
    "DATASET_PATH = r\"C:\\Users\\ncc333\\Desktop\\My_Task\\fine tuning\\New folder\\fine_tune_dataset.jsonl\"\n",
    "\n",
    "# Output directory for trained model\n",
    "OUTPUT_DIR = \"trained_model\"\n",
    "\n",
    "# Model to fine-tune (you can change this)\n",
    "MODEL_NAME = \"microsoft/DialoGPT-medium\"\n",
    "\n",
    "# Training settings (adjust based on your GPU)\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "# SETUP LOGGING\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# MAIN SCRIPT\n",
    "\n",
    "def main():\n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(\"SIMPLE FINE-TUNING SCRIPT\")\n",
    "    logger.info(\"=\" * 80)\n",
    "    \n",
    "    # Step 1: Check dataset\n",
    "    logger.info(f\" Checking dataset: {DATASET_PATH}\")\n",
    "    if not Path(DATASET_PATH).exists():\n",
    "        logger.error(f\" Dataset not found: {DATASET_PATH}\")\n",
    "        logger.error(\"Please update DATASET_PATH in the script!\")\n",
    "        return\n",
    "    \n",
    "    with open(DATASET_PATH, 'r', encoding='utf-8') as f:\n",
    "        num_samples = len(f.readlines())\n",
    "    logger.info(f\"✓ Found {num_samples} samples\")\n",
    "    \n",
    "    # Step 2: Load model and tokenizer\n",
    "    logger.info(f\" Loading model: {MODEL_NAME}\")\n",
    "    \n",
    "    # Use 4-bit quantization to save memory\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    logger.info(\"✓ Model loaded\")\n",
    "    \n",
    "    # Step 3: Setup LoRA\n",
    "    logger.info(\" Setting up LoRA adapters\")\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        bias=\"none\"\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # Step 4: Prepare dataset\n",
    "    logger.info(\" Preparing dataset\")\n",
    "    \n",
    "    def format_and_tokenize(example):\n",
    "        # Handle different JSON formats\n",
    "        if \"text\" in example:\n",
    "            text = example[\"text\"]\n",
    "        elif \"instruction\" in example and \"output\" in example:\n",
    "            text = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['output']}\"\n",
    "        elif \"prompt\" in example and \"completion\" in example:\n",
    "            text = f\"{example['prompt']}\\n{example['completion']}\"\n",
    "        else:\n",
    "            # Fallback: join all string values\n",
    "            text = \" \".join([str(v) for v in example.values() if isinstance(v, str)])\n",
    "        \n",
    "        # Tokenize\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=False\n",
    "        )\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "        return tokenized\n",
    "    \n",
    "    # Load and process dataset\n",
    "    dataset = Dataset.from_json(DATASET_PATH)\n",
    "    dataset = dataset.map(\n",
    "        format_and_tokenize,\n",
    "        remove_columns=dataset.column_names,\n",
    "        desc=\"Tokenizing\"\n",
    "    )\n",
    "    logger.info(f\"✓ Dataset prepared: {len(dataset)} samples\")\n",
    "    \n",
    "    # Step 5: Setup training\n",
    "    logger.info(\" Starting training\")\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=8,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        fp16=True,\n",
    "        logging_steps=10,\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        report_to=\"none\",  # No external logging\n",
    "    )\n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    # Step 6: Train!\n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(\"TRAINING STARTED\")\n",
    "    logger.info(\"=\" * 80)\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Step 7: Save model\n",
    "    logger.info(\" Saving model\")\n",
    "    final_path = Path(OUTPUT_DIR) / \"final\"\n",
    "    trainer.save_model(str(final_path))\n",
    "    tokenizer.save_pretrained(str(final_path))\n",
    "    \n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(\" TRAINING COMPLETE!\")\n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(f\"Model saved to: {final_path}\")\n",
    "    logger.info(\"\\nTo use your model:\")\n",
    "    logger.info(\"1. Load it with: AutoModelForCausalLM.from_pretrained('trained_model/final')\")\n",
    "    logger.info(\"2. Or continue to the inference script below\")\n",
    "\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"Simple test function to try your trained model\"\"\"\n",
    "    logger.info(\"\\n\" + \"=\" * 80)\n",
    "    logger.info(\"TESTING TRAINED MODEL\")\n",
    "    logger.info(\"=\" * 80)\n",
    "    \n",
    "    model_path = Path(OUTPUT_DIR) / \"final\"\n",
    "    \n",
    "    if not model_path.exists():\n",
    "        logger.error(\" Model not found. Train first!\")\n",
    "        return\n",
    "    \n",
    "    # Load model\n",
    "    logger.info(\"Loading model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(str(model_path))\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        str(model_path),\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    model.eval()\n",
    "    \n",
    "    # Test prompt\n",
    "    prompt = \"Hello! How can I help you today?\"\n",
    "    logger.info(f\"\\nPrompt: {prompt}\")\n",
    "    \n",
    "    # Generate\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    logger.info(f\"Response: {response}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run training\n",
    "    main()\n",
    "    \n",
    "    # Uncomment to test after training:\n",
    "    # test_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
